{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pandas \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.callbacks import History,LearningRateScheduler\n",
    "from tensorflow.keras.layers import Dropout,Dense,Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'id'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-0c5305726ebc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtestset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data/test set values.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror_bad_lines\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mfinalfile_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtestset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mdataframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatalabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, other, on, how, lsuffix, rsuffix, sort)\u001b[0m\n\u001b[1;32m   6813\u001b[0m         \u001b[0;31m# For SparseDataFrame's benefit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6814\u001b[0m         return self._join_compat(other, on=on, how=how, lsuffix=lsuffix,\n\u001b[0;32m-> 6815\u001b[0;31m                                  rsuffix=rsuffix, sort=sort)\n\u001b[0m\u001b[1;32m   6816\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6817\u001b[0m     def _join_compat(self, other, on=None, how='left', lsuffix='', rsuffix='',\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_join_compat\u001b[0;34m(self, other, on, how, lsuffix, rsuffix, sort)\u001b[0m\n\u001b[1;32m   6828\u001b[0m             return merge(self, other, left_on=on, how=how,\n\u001b[1;32m   6829\u001b[0m                          \u001b[0mleft_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mon\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6830\u001b[0;31m                          suffixes=(lsuffix, rsuffix), sort=sort)\n\u001b[0m\u001b[1;32m   6831\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6832\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mon\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36mmerge\u001b[0;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m     45\u001b[0m                          \u001b[0mright_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mright_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msort\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msuffixes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msuffixes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m                          \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindicator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindicator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m                          validate=validate)\n\u001b[0m\u001b[1;32m     48\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, left, right, how, on, left_on, right_on, axis, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m    527\u001b[0m         (self.left_join_keys,\n\u001b[1;32m    528\u001b[0m          \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mright_join_keys\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 529\u001b[0;31m          self.join_names) = self._get_merge_keys()\n\u001b[0m\u001b[1;32m    530\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;31m# validate the merge keys dtypes. We may need to coerce\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36m_get_merge_keys\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    856\u001b[0m                     \u001b[0mjoin_names\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 858\u001b[0;31m                     \u001b[0mleft_keys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_label_or_level_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    859\u001b[0m                     \u001b[0mjoin_names\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    860\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mright\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMultiIndex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_get_label_or_level_values\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1704\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_level_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1705\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1706\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1708\u001b[0m         \u001b[0;31m# Check for duplicates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'id'"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "dataset = pd.read_csv(\"data/training set values.csv\", sep=',', error_bad_lines=False, header=0, index_col=0)\n",
    "datalabel = pd.read_csv(\"data/training set labels.csv\", sep=\",\", error_bad_lines=False, header=0, index_col=0)\n",
    "testset = pd.read_csv(\"data/test set values.csv\", sep=',', error_bad_lines=False, header=0, index_col=0)\n",
    "finalfile_index=testset.index\n",
    "dataframe = dataset.join(datalabel, on=[\"id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Add some columns\n",
    "dataframe['functional'] = (dataframe['status_group'] == \"functional\") * 1\n",
    "dataframe['non_functional'] = (dataframe['status_group'] == \"non functional\") * 1\n",
    "dataframe['functional_needs_repair'] = (dataframe['status_group'] == \"functional needs repair\") * 1\n",
    "dataframe['categorical_population'] = pd.qcut(dataframe['population'][dataframe['population'] > 0], 4)\n",
    "dataframe['categorical_gps_height'] = pd.qcut(dataframe['gps_height'][dataframe['gps_height'] > 0], 4)\n",
    "dataframe['categorical_construction_year'] = pd.qcut(dataframe['construction_year'][dataframe['construction_year'] > 0], 4)\n",
    "dataframe.loc[dataframe['construction_year'] == 0, 'construction_year'] = dataframe['construction_year'].min()\n",
    "dataframe['categorical_usage_time'] = dataframe.apply(lambda row: int(row.date_recorded.split('-')[2]) - row.construction_year, axis=1)\n",
    "dataframe['categorical_usage_time'] = pd.qcut(dataframe['categorical_usage_time'][dataframe['categorical_usage_time'] > 0], 4)\n",
    "\n",
    "testset['categorical_population'] = pd.qcut(testset['population'][testset['population'] > 0], 4)\n",
    "testset['categorical_gps_height'] = pd.qcut(testset['gps_height'][testset['gps_height'] > 0], 4)\n",
    "testset['categorical_construction_year'] = pd.qcut(testset['construction_year'][testset['construction_year'] > 0], 4)\n",
    "testset.loc[testset['construction_year'] == 0, 'construction_year'] = testset['construction_year'].min()\n",
    "testset['categorical_usage_time'] = testset.apply(lambda row: int(row.date_recorded.split('-')[2]) - row.construction_year, axis=1)\n",
    "testset['categorical_usage_time'] = pd.qcut(testset['categorical_usage_time'][testset['categorical_usage_time'] > 0], 4)\n",
    "\n",
    "dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Geographic location.sort_values(by=('functional'))\n",
    "\n",
    "print(dataframe[['region', 'functional', 'non_functional', 'functional_needs_repair']].groupby(['region']).mean().sort_values(by=('functional')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How the waterpoint is managed\n",
    "\n",
    "print(dataframe[['management', 'functional', 'non_functional', 'functional_needs_repair']].groupby(['management']).mean().sort_values(by=('functional')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The kind of extraction the waterpoint uses\n",
    "\n",
    "print(dataframe[['extraction_type_class', 'functional', 'non_functional', 'functional_needs_repair']].groupby(['extraction_type_class']).mean().sort_values(by=('functional')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What the water costs\n",
    "\n",
    "print(dataframe[['payment', 'functional', 'non_functional', 'functional_needs_repair']].groupby(['payment']).mean().sort_values(by=('functional')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The quality of the water\n",
    "\n",
    "print(dataframe[['quality_group', 'functional', 'non_functional', 'functional_needs_repair']].groupby(['quality_group']).mean().sort_values(by=('functional')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The quantity of water\n",
    "\n",
    "print(dataframe[['quantity', 'functional', 'non_functional', 'functional_needs_repair']]\n",
    "      .groupby(['quantity'])\n",
    "      .mean()\n",
    "      .sort_values(by=('functional'))\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The source of the water\n",
    "\n",
    "print(dataframe[['source_type', 'functional', 'non_functional', 'functional_needs_repair']]\n",
    "      .groupby(['source_type'])\n",
    "      .mean()\n",
    "      .sort_values(by=('functional'))\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The population number\n",
    "\n",
    "print(dataframe[['categorical_population', 'functional', 'non_functional', 'functional_needs_repair']]\n",
    "      .groupby(['categorical_population'])\n",
    "      .mean()\n",
    "      .sort_values(by=('functional'))\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The gps height\n",
    "\n",
    "print(dataframe[['categorical_gps_height', 'functional', 'non_functional', 'functional_needs_repair']]\n",
    "      .groupby(['categorical_gps_height'])\n",
    "      .mean()\n",
    "      .sort_values(by=('functional'))\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The construction year\n",
    "\n",
    "print(dataframe[['categorical_construction_year', 'functional', 'non_functional', 'functional_needs_repair']]\n",
    "      .groupby(['categorical_construction_year'])\n",
    "      .mean()\n",
    "      .sort_values(by=('functional'))\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The old usage\n",
    "\n",
    "print(dataframe[['categorical_usage_time', 'functional', 'non_functional', 'functional_needs_repair']]\n",
    "      .groupby(['categorical_usage_time'])\n",
    "      .mean()\n",
    "      .sort_values(by=('functional'))\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop some columns\n",
    "dataframe.drop([\n",
    "    \"funder\",\n",
    "    \"gps_height\",\n",
    "    \"installer\",\n",
    "    \"longitude\",\n",
    "    \"latitude\",\n",
    "    \"wpt_name\",\n",
    "    \"num_private\",\n",
    "    \"basin\",\n",
    "    \"subvillage\",\n",
    "    \"region_code\",\n",
    "    \"district_code\",\n",
    "    \"lga\",\n",
    "    \"ward\",\n",
    "    \"population\",\n",
    "    \"public_meeting\",\n",
    "    \"recorded_by\",\n",
    "    \"scheme_management\",\n",
    "    \"extraction_type_group\",\n",
    "    \"management_group\",\n",
    "    \"payment\",\n",
    "    \"payment_type\",\n",
    "    \"water_quality\",\n",
    "    \"quantity_group\",\n",
    "    \"source\",\n",
    "    \"source_class\",\n",
    "    \"waterpoint_type\",\n",
    "    \"scheme_name\",\n",
    "    \"permit\",\n",
    "    \"construction_year\",\n",
    "    \"extraction_type\",\n",
    "    \"waterpoint_type_group\",\n",
    "    \"date_recorded\",\n",
    "    \"amount_tsh\",\n",
    "    \"functional\",\n",
    "    \"non_functional\",\n",
    "    \"functional_needs_repair\"\n",
    "], inplace=True, axis=1)\n",
    "testset.drop([\n",
    "    \"funder\",\n",
    "    \"gps_height\",\n",
    "    \"installer\",\n",
    "    \"longitude\",\n",
    "    \"latitude\",\n",
    "    \"wpt_name\",\n",
    "    \"num_private\",\n",
    "    \"basin\",\n",
    "    \"subvillage\",\n",
    "    \"region_code\",\n",
    "    \"district_code\",\n",
    "    \"lga\",\n",
    "    \"ward\",\n",
    "    \"population\",\n",
    "    \"public_meeting\",\n",
    "    \"recorded_by\",\n",
    "    \"scheme_management\",\n",
    "    \"extraction_type_group\",\n",
    "    \"management_group\",\n",
    "    \"payment\",\n",
    "    \"payment_type\",\n",
    "    \"water_quality\",\n",
    "    \"quantity_group\",\n",
    "    \"source\",\n",
    "    \"source_class\",\n",
    "    \"waterpoint_type\",\n",
    "    \"scheme_name\",\n",
    "    \"permit\",\n",
    "    \"construction_year\",\n",
    "    \"extraction_type\",\n",
    "    \"waterpoint_type_group\",\n",
    "    \"date_recorded\",\n",
    "    \"amount_tsh\",\n",
    "], inplace=True, axis=1)\n",
    "\n",
    "# Transform non numeric values to numeric values\n",
    "nonNumericKeys = [\n",
    "    'region', 'management', 'extraction_type_class', 'quality_group', 'quantity', 'source_type', \n",
    "    'categorical_population', 'categorical_gps_height', 'categorical_construction_year', 'categorical_usage_time',\n",
    "]\n",
    "for nonNumericKey in nonNumericKeys:\n",
    "    print(dataframe[nonNumericKey].unique().tolist())\n",
    "    \n",
    "    # Transformation\n",
    "    for (i, value) in enumerate(dataframe[nonNumericKey].unique().tolist()):\n",
    "        dataframe[nonNumericKey].replace(value, i, inplace=True)\n",
    "    for (i, value) in enumerate(testset[nonNumericKey].unique().tolist()):\n",
    "        testset[nonNumericKey].replace(value, i, inplace=True)\n",
    "    \n",
    "    # Convert column to int\n",
    "    dataframe[nonNumericKey] = dataframe[nonNumericKey].astype(int)\n",
    "    testset[nonNumericKey] = testset[nonNumericKey].astype(int)\n",
    "    \n",
    "# Transformation for status_group\n",
    "# Convert column to int\n",
    "dataframe['status_group'].replace('functional', 1, inplace=True)\n",
    "dataframe['status_group'].replace('functional needs repair', 0, inplace=True)\n",
    "dataframe['status_group'].replace('non functional', -1, inplace=True)\n",
    "dataframe['status_group'] = dataframe['status_group'].astype(int)\n",
    "\n",
    "dataframe = dataframe.reindex(columns=(['status_group'] + list([a for a in dataframe.columns if a != 'status_group']) ))\n",
    "    \n",
    "dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "#Séparation des valeurs de train et label (tous les exemples)\n",
    "X_alltrain = dataframe.values[:, 1:]\n",
    "y_alltrain = dataframe.values[:, 0]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_alltrain, y_alltrain, random_state=42)\n",
    "print('%i X_train, %i X_test, %i y_train,%i y_test'%(\n",
    "    X_train.shape[0], X_test.shape[0], y_train.shape[0], y_test.shape[0]))\n",
    "print('%s X_alltrain, %s y_alltrain'%(X_alltrain.shape, y_alltrain.shape))\n",
    "print(y_alltrain)\n",
    "feature_names=dataframe.columns.tolist()[1:]\n",
    "target_names=[\"Non fonctionnel\",\"Fonctionnel\"]\n",
    "print('features:',feature_names)\n",
    "print('target:',target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonctions permettant de générer le fichier d'envoi à Kaggle\n",
    "#\n",
    "#parametres: Classifiers; Données à calculer ; index)\n",
    "def generer_resultats(clf,data=dataframe.values):\n",
    "    \"\"\"\n",
    "    Fonctions permettant de générer le fichier d'envoi à Kaggle.\n",
    "    On passe un classifier sur lequel on refait le training avec toutes les données de training\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    Classifiers : Classifier utilisé pour la prédiction\n",
    "    data : Données à calculer. par défaut, les valeurs du dataset \"test\"\n",
    "    \"\"\" \n",
    "    print(clf.get_params())\n",
    "    clf.fit(X_alltrain, y_alltrain)\n",
    "    prediction=clf.predict(data)\n",
    "    results=pd.DataFrame(prediction.astype(int), index = finalfile_index, columns=['status_group'])\n",
    "    results['status_group'].replace(1, 'functional', inplace=True)\n",
    "    results['status_group'].replace(0, 'functional needs repair', inplace=True)\n",
    "    results['status_group'].replace(-1, 'non functional', inplace=True)\n",
    "    results.to_csv('resultats%s.csv'%clf.__class__.__name__)\n",
    "    \n",
    "#Fonction pour l'affichage 2 D des résultats    \n",
    "from matplotlib.colors import ListedColormap\n",
    "def plot_decision_boundary(clf,X,y, axes=[-0, 30, -5, 5], axis_name=['x1','x2'],alpha=0.5, contour=True):\n",
    "    \"\"\"\n",
    "    Fonction pour l'affichage 2 D des résultats   \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    clf : Classifier à afficher\n",
    "    X : features de Données a afficher\n",
    "    y : labels de Données a afficher  \n",
    "    axes : : Tailles des axes (valeur min/max)\n",
    "    axis_name : Nom des axes sur le graphique\n",
    "    alpha : Transparence des points\n",
    "    contour : Afichage du contour\n",
    "    \"\"\"     \n",
    "    x1s = np.linspace(axes[0], axes[1], 100)\n",
    "    x2s = np.linspace(axes[2], axes[3], 100)\n",
    "    x1, x2 = np.meshgrid(x1s, x2s)\n",
    "    X_new = np.c_[x1.ravel(), x2.ravel()]\n",
    "    y_pred = clf.predict(X_new).reshape(x1.shape)\n",
    "    custom_cmap = ListedColormap(['#fafab0','#9898ff','#a0faa0'])\n",
    "    plt.contourf(x1, x2, y_pred, alpha=0.3, cmap=custom_cmap)\n",
    "    if contour:\n",
    "        custom_cmap2 = ListedColormap(['#7d7d58','#4c4c7f','#507d50'])\n",
    "        plt.contour(x1, x2, y_pred, cmap=custom_cmap2, alpha=0.8)\n",
    "    plt.plot(X[:, 0][y==0], X[:, 1][y==0], \"bo\",label=\"Non fonctionnel\", alpha=alpha)\n",
    "    plt.plot(X[:, 0][y==1], X[:, 1][y==1], \"ys\", label=\"Fonctionnel\",alpha=alpha)\n",
    "    plt.axis(axes)\n",
    "    plt.xlabel(axis_name[0], fontsize=18)\n",
    "    plt.ylabel(axis_name[1]+ \"  \",fontsize=18, rotation=0)    \n",
    "    plt.legend(loc=\"lower right\", fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB,MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "gnb_clf = RandomForestClassifier(random_state=2)\n",
    "y_train = y_train.astype(\"int\")\n",
    "gnb_clf.fit(X_train, y_train)\n",
    "y_pred = gnb_clf.predict(X_test)    \n",
    "print(\"Niveau de précision : %.2f\"%(100*accuracy_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graphes\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Machine learning\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "#Eviter les warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning) \n",
    "\n",
    "rnd_clf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Lance sur les données de test\n",
    "generer_resultats(rnd_clf, testset.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
