{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP : Naïve Bayes et Ensemble appliqués au Titanic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Récupération des données du cours ANN\n",
    "Renvoie 2 Dataframe train (avec le champs Survived) et test (sans le champs Survived)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<span style=\"color:blue;font-weight:bold\">train :</span>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 12 columns):\n",
      "PassengerId    891 non-null int64\n",
      "Survived       891 non-null int64\n",
      "Pclass         891 non-null int64\n",
      "Name           891 non-null object\n",
      "Sex            891 non-null object\n",
      "Age            714 non-null float64\n",
      "SibSp          891 non-null int64\n",
      "Parch          891 non-null int64\n",
      "Ticket         891 non-null object\n",
      "Fare           891 non-null float64\n",
      "Cabin          204 non-null object\n",
      "Embarked       889 non-null object\n",
      "dtypes: float64(2), int64(5), object(5)\n",
      "memory usage: 83.6+ KB\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<span style=\"color:blue;font-weight:bold\">test :</span>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 418 entries, 0 to 417\n",
      "Data columns (total 11 columns):\n",
      "PassengerId    418 non-null int64\n",
      "Pclass         418 non-null int64\n",
      "Name           418 non-null object\n",
      "Sex            418 non-null object\n",
      "Age            332 non-null float64\n",
      "SibSp          418 non-null int64\n",
      "Parch          418 non-null int64\n",
      "Ticket         418 non-null object\n",
      "Fare           417 non-null float64\n",
      "Cabin          91 non-null object\n",
      "Embarked       418 non-null object\n",
      "dtypes: float64(2), int64(4), object(5)\n",
      "memory usage: 36.0+ KB\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<span style=\"color:blue;font-weight:bold\">Effet de la classe</span>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Pclass  Survived\n",
      "0       1  0.629630\n",
      "1       2  0.472826\n",
      "2       3  0.242363\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<span style=\"color:blue;font-weight:bold\">Effet du Genre</span>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Sex  Survived\n",
      "0  female  0.742038\n",
      "1    male  0.188908\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<span style=\"color:blue;font-weight:bold\">Effet de la taille de la famille</span>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   FamilySize  Survived\n",
      "0           1  0.303538\n",
      "1           2  0.552795\n",
      "2           3  0.578431\n",
      "3           4  0.724138\n",
      "4           5  0.200000\n",
      "5           6  0.136364\n",
      "6           7  0.333333\n",
      "7           8  0.000000\n",
      "8          11  0.000000\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<span style=\"color:blue;font-weight:bold\">Effet de la présence de la famille</span>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   IsAlone  Survived\n",
      "0        0  0.505650\n",
      "1        1  0.303538\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<span style=\"color:blue;font-weight:bold\">Effet du Port d'embarquement</span>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Embarked  Survived\n",
      "0        C  0.553571\n",
      "1        Q  0.389610\n",
      "2        S  0.339009\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<span style=\"color:blue;font-weight:bold\">Effet du Prix du Ticket</span>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   CategoricalFare  Survived\n",
      "0   (-0.001, 7.91]  0.197309\n",
      "1   (7.91, 14.454]  0.303571\n",
      "2   (14.454, 31.0]  0.454955\n",
      "3  (31.0, 512.329]  0.581081\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<span style=\"color:blue;font-weight:bold\">Effet de l'age</span>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     CategoricalAge  Survived\n",
      "0    (0.34, 16.336]  0.543860\n",
      "1  (16.336, 32.252]  0.347126\n",
      "2  (32.252, 48.168]  0.374046\n",
      "3  (48.168, 64.084]  0.434783\n",
      "4    (64.084, 80.0]  0.090909\n",
      "Sex       female  male\n",
      "Title                 \n",
      "Capt           0     1\n",
      "Col            0     2\n",
      "Countess       1     0\n",
      "Don            0     1\n",
      "Dr             1     6\n",
      "Jonkheer       0     1\n",
      "Lady           1     0\n",
      "Major          0     2\n",
      "Master         0    40\n",
      "Miss         182     0\n",
      "Mlle           2     0\n",
      "Mme            1     0\n",
      "Mr             0   517\n",
      "Mrs          125     0\n",
      "Ms             1     0\n",
      "Rev            0     6\n",
      "Sir            0     1\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<span style=\"color:blue;font-weight:bold\">Effet du titre</span>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Title  Survived\n",
      "0  Master  0.575000\n",
      "1    Miss  0.702703\n",
      "2      Mr  0.156673\n",
      "3     Mrs  0.793651\n",
      "4    Rare  0.347826\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<span style=\"color:blue;font-weight:bold\">train mis en forme :</span>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 8 columns):\n",
      "Survived    891 non-null int64\n",
      "Pclass      891 non-null int64\n",
      "Sex         891 non-null int32\n",
      "Age         891 non-null float64\n",
      "Fare        891 non-null float64\n",
      "Embarked    891 non-null int32\n",
      "IsAlone     891 non-null int64\n",
      "Title       891 non-null int64\n",
      "dtypes: float64(2), int32(2), int64(4)\n",
      "memory usage: 48.8 KB\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<span style=\"color:blue;font-weight:bold\">test mis en forme :</span>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 418 entries, 0 to 417\n",
      "Data columns (total 7 columns):\n",
      "Pclass      418 non-null int64\n",
      "Sex         418 non-null int32\n",
      "Age         418 non-null float64\n",
      "Fare        418 non-null float64\n",
      "Embarked    418 non-null int32\n",
      "IsAlone     418 non-null int64\n",
      "Title       418 non-null int64\n",
      "dtypes: float64(2), int32(2), int64(3)\n",
      "memory usage: 19.7 KB\n"
     ]
    }
   ],
   "source": [
    "%run ./tp1_prepa_features.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Préparation des données de Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "668 X_train, 223 X_test, 668 y_train,223 y_test\n",
      "891 X_alltrain, 891 y_alltrain\n",
      "features: ['Pclass', 'Sex', 'Age', 'Fare', 'Embarked', 'IsAlone', 'Title']\n",
      "target: ['Disparu', 'Rescapé']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "#Séparation des valeurs de train et label (tous les exemples)\n",
    "X_alltrain = train.values[:, 1:]\n",
    "y_alltrain = train.values[:, 0]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_alltrain, y_alltrain, random_state=42)\n",
    "print('%i X_train, %i X_test, %i y_train,%i y_test'%(\n",
    "    X_train.shape[0], X_test.shape[0], y_train.shape[0], y_test.shape[0]))\n",
    "print('%i X_alltrain, %i y_alltrain'%(X_alltrain.shape[0], y_alltrain.shape[0]))\n",
    "feature_names=train.columns.tolist()[1:]\n",
    "target_names=[\"Disparu\",\"Rescapé\"]\n",
    "print('features:',feature_names)\n",
    "print('target:',target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fonction Utilitaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonctions permettant de générer le fichier d'envoi à Kaggle\n",
    "#\n",
    "#parametres: Classifiers; Données à calculer ; index)\n",
    "def generer_resultats(clf,data=test.values,idx=finalfile_index):\n",
    "    \"\"\"\n",
    "    Fonctions permettant de générer le fichier d'envoi à Kaggle.\n",
    "    On passe un classifier sur lequel on refait le training avec toutes les données de training\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    Classifiers : Classifier utilisé pour la prédiction\n",
    "    data : Données à calculer. par défaut, les valeurs du dataset \"test\"\n",
    "    idx : Index des passagers testés. Stockés dans finalfile_index lors de la lecture des données\n",
    "    \"\"\"    \n",
    "    print(clf.get_params())\n",
    "    clf.fit(X_alltrain, y_alltrain)\n",
    "    prediction=clf.predict(data)\n",
    "    results=pd.DataFrame(prediction.astype(int), index = finalfile_index, columns=['Survived'])\n",
    "    results.to_csv('resultats%s.csv'%clf.__class__.__name__)\n",
    "    \n",
    "#Fonction pour l'affichage 2 D des résultats    \n",
    "from matplotlib.colors import ListedColormap\n",
    "def plot_decision_boundary(clf,X,y, axes=[-0, 30, -5, 5], axis_name=['x1','x2'],alpha=0.5, contour=True):\n",
    "    \"\"\"\n",
    "    Fonction pour l'affichage 2 D des résultats   \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    clf : Classifier à afficher\n",
    "    X : features de Données a afficher\n",
    "    y : labels de Données a afficher  \n",
    "    axes : : Tailles des axes (valeur min/max)\n",
    "    axis_name : Nom des axes sur le graphique\n",
    "    alpha : Transparence des points\n",
    "    contour : Afichage du contour\n",
    "    \"\"\"     \n",
    "    x1s = np.linspace(axes[0], axes[1], 100)\n",
    "    x2s = np.linspace(axes[2], axes[3], 100)\n",
    "    x1, x2 = np.meshgrid(x1s, x2s)\n",
    "    X_new = np.c_[x1.ravel(), x2.ravel()]\n",
    "    y_pred = clf.predict(X_new).reshape(x1.shape)\n",
    "    custom_cmap = ListedColormap(['#fafab0','#9898ff','#a0faa0'])\n",
    "    plt.contourf(x1, x2, y_pred, alpha=0.3, cmap=custom_cmap)\n",
    "    if contour:\n",
    "        custom_cmap2 = ListedColormap(['#7d7d58','#4c4c7f','#507d50'])\n",
    "        plt.contour(x1, x2, y_pred, cmap=custom_cmap2, alpha=0.8)\n",
    "    plt.plot(X[:, 0][y==0], X[:, 1][y==0], \"bo\",label=\"Disparu\", alpha=alpha)\n",
    "    plt.plot(X[:, 0][y==1], X[:, 1][y==1], \"ys\", label=\"Rescapé\",alpha=alpha)\n",
    "    plt.axis(axes)\n",
    "    plt.xlabel(axis_name[0], fontsize=18)\n",
    "    plt.ylabel(axis_name[1]+ \"  \",fontsize=18, rotation=0)    \n",
    "    plt.legend(loc=\"lower right\", fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice 1 : Naïve Bayes\n",
    "En utilisant l'exemple du cours 2, contruisez et testez un modèle bayésien pour prévoir la survie. <br>\n",
    "Essayez plusieurs valeurs pour max_depth <br>\n",
    "Envoyer à Kaggle en utilisant la fonction generer_resultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Niveau de précision : 77.13\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB,MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "gnb_clf = GaussianNB()\n",
    "gnb_clf.fit(X_train, y_train)\n",
    "y_pred = gnb_clf.predict(X_test)    \n",
    "print(\"Niveau de précision : %.2f\"%(100*accuracy_score(y_test, y_pred)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice 2 : Modèles d'ensembles\n",
    "En utilisant les l'exemple 2 du cours 1 ainsi que les résultat du TP1, créer un modèle d'ensemble en testant soft et hard voting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'estimators': [('lr', LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=42, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)), ('rf', RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=42, verbose=0, warm_start=False)), ('by', GaussianNB(priors=None))], 'flatten_transform': None, 'n_jobs': 1, 'voting': 'soft', 'weights': None, 'lr': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=42, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), 'rf': RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=42, verbose=0, warm_start=False), 'by': GaussianNB(priors=None), 'lr__C': 1.0, 'lr__class_weight': None, 'lr__dual': False, 'lr__fit_intercept': True, 'lr__intercept_scaling': 1, 'lr__max_iter': 100, 'lr__multi_class': 'ovr', 'lr__n_jobs': 1, 'lr__penalty': 'l2', 'lr__random_state': 42, 'lr__solver': 'liblinear', 'lr__tol': 0.0001, 'lr__verbose': 0, 'lr__warm_start': False, 'rf__bootstrap': True, 'rf__class_weight': None, 'rf__criterion': 'gini', 'rf__max_depth': None, 'rf__max_features': 'auto', 'rf__max_leaf_nodes': None, 'rf__min_impurity_decrease': 0.0, 'rf__min_impurity_split': None, 'rf__min_samples_leaf': 1, 'rf__min_samples_split': 2, 'rf__min_weight_fraction_leaf': 0.0, 'rf__n_estimators': 10, 'rf__n_jobs': 1, 'rf__oob_score': False, 'rf__random_state': 42, 'rf__verbose': 0, 'rf__warm_start': False, 'by__priors': None}\n"
     ]
    }
   ],
   "source": [
    "# Graphes\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Machine learning\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "#Eviter les warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "\n",
    "log_clf = LogisticRegression(random_state=42)\n",
    "rnd_clf = RandomForestClassifier(random_state=42)\n",
    "bayes_clf = GaussianNB()\n",
    "\n",
    "voting_clf = VotingClassifier(estimators=[('lr', log_clf),\n",
    "                                          ('rf', rnd_clf), \n",
    "                                          ('by', bayes_clf)],\n",
    "                              voting='hard')\n",
    "\n",
    "dict_scores={}\n",
    "for clf in (log_clf, rnd_clf, bayes_clf, voting_clf):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)    \n",
    "    dict_scores[clf.__class__.__name__]=accuracy_score(y_test, y_pred)\n",
    "    \n",
    "voting_soft_clf = VotingClassifier(estimators=[('lr', log_clf),\n",
    "                                          ('rf', rnd_clf),\n",
    "                                          ('by', bayes_clf)],\n",
    "                              voting='soft')\n",
    "voting_soft_clf.fit(X_train, y_train)\n",
    "y_pred = voting_soft_clf.predict(X_test)\n",
    "dict_scores['VotingClassifier Soft']=accuracy_score(y_test, y_pred)\n",
    "dict_scores\n",
    "\n",
    "generer_resultats(voting_soft_clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice 3 Optionnel : Stacking\n",
    "Remplacer le voting par un modèle logistique (voir exemple 2 cours 1)\n",
    "Vous pouvez utiliser la cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3-fold cross validation:\n",
      "\n",
      "Accuracy: 0.91 (+/- 0.01) [KNN]\n",
      "Accuracy: 0.91 (+/- 0.06) [Random Forest]\n",
      "Accuracy: 0.92 (+/- 0.03) [Naive Bayes]\n",
      "Accuracy: 0.95 (+/- 0.03) [StackingClassifier]\n",
      "{'kneighborsclassifier': KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=1, p=2,\n",
      "           weights='uniform'), 'randomforestclassifier': RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=1, verbose=0, warm_start=False), 'gaussiannb': GaussianNB(priors=None), 'kneighborsclassifier__algorithm': 'auto', 'kneighborsclassifier__leaf_size': 30, 'kneighborsclassifier__metric': 'minkowski', 'kneighborsclassifier__metric_params': None, 'kneighborsclassifier__n_jobs': 1, 'kneighborsclassifier__n_neighbors': 1, 'kneighborsclassifier__p': 2, 'kneighborsclassifier__weights': 'uniform', 'randomforestclassifier__bootstrap': True, 'randomforestclassifier__class_weight': None, 'randomforestclassifier__criterion': 'gini', 'randomforestclassifier__max_depth': None, 'randomforestclassifier__max_features': 'auto', 'randomforestclassifier__max_leaf_nodes': None, 'randomforestclassifier__min_impurity_decrease': 0.0, 'randomforestclassifier__min_impurity_split': None, 'randomforestclassifier__min_samples_leaf': 1, 'randomforestclassifier__min_samples_split': 2, 'randomforestclassifier__min_weight_fraction_leaf': 0.0, 'randomforestclassifier__n_estimators': 10, 'randomforestclassifier__n_jobs': 1, 'randomforestclassifier__oob_score': False, 'randomforestclassifier__random_state': 1, 'randomforestclassifier__verbose': 0, 'randomforestclassifier__warm_start': False, 'gaussiannb__priors': None, 'meta-logisticregression': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), 'meta-logisticregression__C': 1.0, 'meta-logisticregression__class_weight': None, 'meta-logisticregression__dual': False, 'meta-logisticregression__fit_intercept': True, 'meta-logisticregression__intercept_scaling': 1, 'meta-logisticregression__max_iter': 100, 'meta-logisticregression__multi_class': 'ovr', 'meta-logisticregression__n_jobs': 1, 'meta-logisticregression__penalty': 'l2', 'meta-logisticregression__random_state': None, 'meta-logisticregression__solver': 'liblinear', 'meta-logisticregression__tol': 0.0001, 'meta-logisticregression__verbose': 0, 'meta-logisticregression__warm_start': False, 'average_probas': False, 'classifiers': [KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=1, p=2,\n",
      "           weights='uniform'), RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=1, verbose=0, warm_start=False), GaussianNB(priors=None)], 'meta_classifier': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), 'store_train_meta_features': False, 'use_clones': True, 'use_features_in_secondary': False, 'use_probas': False, 'verbose': 0}\n"
     ]
    }
   ],
   "source": [
    "from mlxtend.classifier import StackingClassifier\n",
    "\n",
    "clf1 = KNeighborsClassifier(n_neighbors=1)\n",
    "clf2 = RandomForestClassifier(random_state=1)\n",
    "\n",
    "lr = LogisticRegression()\n",
    "sclf = StackingClassifier(classifiers=[clf1, clf2, gnb_clf], \n",
    "                          meta_classifier=lr)\n",
    "\n",
    "print('3-fold cross validation:\\n')\n",
    "\n",
    "for clf, label in zip([clf1, clf2, gnb_clf, sclf], \n",
    "                      ['KNN', \n",
    "                       'Random Forest', \n",
    "                       'Naive Bayes',\n",
    "                       'StackingClassifier']):\n",
    "\n",
    "    scores = model_selection.cross_val_score(clf, X, y, \n",
    "                                              cv=3, scoring='accuracy')\n",
    "    print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" \n",
    "          % (scores.mean(), scores.std(), label))\n",
    "    \n",
    "generer_resultats(sclf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice 4 Optionnel : Ajouter les prédictions tensorflow du cours Neural Network au modèle d'ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
